{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats import uniform\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import sklearn\n",
    "import math\n",
    "from scipy import stats\n",
    "import openpyxl\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, LeaveOneGroupOut, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFpr, SelectFwe, RFECV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegression, SGDClassifier, LogisticRegressionCV, LarsCV, LassoLarsCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATASET \n",
    "##### ONLY LMCI/EMCI SUBJECTS; \n",
    "##### BASELINE INFORMATION + CONVERSION AT 3-YEARS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"../../adnimerge_screen.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_scree_out = [\"PTID\", \"VISCODE\", \"COLPROT\", \"ORIGPROT\", \"EXAMDATE\", \"PTETHCAT\", \"PTRACCAT\", \"FDG\", \"PIB\", \"AV45\", \"ABETA\", \"TAU\", \"PTAU\", \"EcogPtMem\", \"MOCA\",\"EcogPtLang\", \"EcogPtVisspat\", \"EcogPtPlan\", \"EcogPtOrgan\", \"EcogPtDivatt\", \"EcogPtTotal\", \"EcogSPMem\", \"EcogSPLang\", \"EcogSPVisspat\", \"EcogSPPlan\", \"EcogSPOrgan\", \"EcogSPDivatt\", \"EcogSPTotal\", \"FLDSTRENG\", \"FSVERSION\", \"Ventricles\", \"Hippocampus\", \"WholeBrain\", \"Entorhinal\", \"Fusiform\", \"MidTemp\", \"ICV\", \"mPACCdigit\", \"mPACCtrailsB\", \"EXAMDATE_bl\", \"CDRSB_bl\", \"ADAS11_bl\", \"ADAS13_bl\", \"ADASQ4_bl\", \"MMSE_bl\", \"RAVLT_immediate_bl\", \"RAVLT_learning_bl\", \"RAVLT_forgetting_bl\", \"RAVLT_perc_forgetting_bl\", \"FAQ_bl\", \"mPACCdigit_bl\", \"mPACCtrailsB_bl\", \"FLDSTRENG_bl\", \"FSVERSION_bl\", \"Ventricles_bl\", \"Hippocampus_bl\", \"WholeBrain_bl\", \"Entorhinal_bl\", \"Fusiform_bl\", \"MidTemp_bl\", \"ICV_bl\", \"MOCA_bl\", \"EcogPtMem_bl\", \"EcogPtLang_bl\", \"EcogPtVisspat_bl\", \"EcogPtPlan_bl\", \"EcogPtOrgan_bl\", \"EcogPtDivatt_bl\", \"EcogPtTotal_bl\", \"EcogSPMem_bl\", \"EcogSPLang_bl\", \"EcogSPVisspat_bl\", \"EcogSPPlan_bl\", \"EcogSPOrgan_bl\", \"EcogSPDivatt_bl\", \"EcogSPTotal_bl\", \"ABETA_bl\", \"TAU_bl\", \"PTAU_bl\", \"FDG_bl\", \"PIB_bl\", \"AV45_bl\", \"Years_bl\", \"Month_bl\", \"Month\", \"M\", \"DX\",\"update_stamp\"]\n",
    "dataset = dataset.drop(variables_to_scree_out, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>PTMARRY</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>...</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>DIGITSCOR</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>CONVERSION_AT_3Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>67.5</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.33</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>36.3636</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>80.4</td>\n",
       "      <td>Female</td>\n",
       "      <td>13</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.67</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>72.8</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>88.8889</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>99</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>66.5</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Married</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.67</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>57.1429</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>99</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>20.33</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>77.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>Married</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15.67</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>77</td>\n",
       "      <td>67</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>79.7</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>19.00</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80</td>\n",
       "      <td>18</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Married</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>14.00</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>42.8571</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>98</td>\n",
       "      <td>67</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>84.4</td>\n",
       "      <td>Female</td>\n",
       "      <td>16</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.67</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>7</td>\n",
       "      <td>38.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>101</td>\n",
       "      <td>7</td>\n",
       "      <td>LMCI</td>\n",
       "      <td>73.6</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Married</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>8</td>\n",
       "      <td>49.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID  SITE DX_bl   AGE PTGENDER  PTEDUCAT   PTMARRY  APOE4  CDRSB  ADAS11  \\\n",
       "0    4    22  LMCI  67.5     Male        10   Married      0    1.0   14.33   \n",
       "1    6   100  LMCI  80.4   Female        13   Married      0    0.5   18.67   \n",
       "2   42    23  LMCI  72.8     Male        18   Married      0    0.5    7.00   \n",
       "3   51    99  LMCI  66.5     Male        18   Married      2    1.0    9.67   \n",
       "4   54    99  LMCI  81.0   Female        20   Married      0    2.5   20.33   \n",
       "5   57    18  LMCI  77.3     Male        20   Married      1    1.5   15.67   \n",
       "6   77    67  LMCI  79.7     Male        18   Married      0    0.5   19.00   \n",
       "7   80    18  LMCI  85.0     Male        18   Married      1    1.5   14.00   \n",
       "8   98    67  LMCI  84.4   Female        16  Divorced      1    2.0   10.67   \n",
       "9  101     7  LMCI  73.6     Male        18   Married      2    0.5    7.00   \n",
       "\n",
       "         ...         MMSE  RAVLT_immediate  RAVLT_learning  RAVLT_forgetting  \\\n",
       "0        ...           27               37               7                 4   \n",
       "1        ...           25               30               1                 5   \n",
       "2        ...           30               29               6                 8   \n",
       "3        ...           27               29               1                 4   \n",
       "4        ...           27               26               3                 6   \n",
       "5        ...           27               29               6                 8   \n",
       "6        ...           28               18               3                 5   \n",
       "7        ...           27               23               4                 3   \n",
       "8        ...           26               25               4                 5   \n",
       "9        ...           27               24               3                 5   \n",
       "\n",
       "   RAVLT_perc_forgetting  LDELTOTAL  DIGITSCOR  TRABSCOR   FAQ  \\\n",
       "0                36.3636          4       25.0     271.0   0.0   \n",
       "1                83.3333          3       34.0     168.0   0.0   \n",
       "2                88.8889          1       47.0     101.0   2.0   \n",
       "3                57.1429          7       50.0      94.0   2.0   \n",
       "4               100.0000          0       20.0     201.0  12.0   \n",
       "5               100.0000          3       20.0     151.0   7.0   \n",
       "6               100.0000          1       35.0     157.0   3.0   \n",
       "7                42.8571          1       48.0      81.0   1.0   \n",
       "8                55.5556          7       38.0     167.0   4.0   \n",
       "9                71.4286          8       49.0      50.0   0.0   \n",
       "\n",
       "   CONVERSION_AT_3Y  \n",
       "0                NO  \n",
       "1                NO  \n",
       "2               YES  \n",
       "3                NO  \n",
       "4               YES  \n",
       "5               YES  \n",
       "6               YES  \n",
       "7                NO  \n",
       "8               YES  \n",
       "9               YES  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOWING A PREVIEW OF THE DATASET\n",
    "\n",
    "display(dataset.iloc[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS11</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>ADASQ4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>DIGITSCOR</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>FAQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>546.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2385.623636</td>\n",
       "      <td>74.014545</td>\n",
       "      <td>73.052545</td>\n",
       "      <td>16.018182</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1.509091</td>\n",
       "      <td>10.198506</td>\n",
       "      <td>16.460037</td>\n",
       "      <td>5.523636</td>\n",
       "      <td>27.594545</td>\n",
       "      <td>34.330909</td>\n",
       "      <td>4.092727</td>\n",
       "      <td>4.665455</td>\n",
       "      <td>60.799743</td>\n",
       "      <td>5.676364</td>\n",
       "      <td>37.653846</td>\n",
       "      <td>115.073260</td>\n",
       "      <td>3.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1775.001570</td>\n",
       "      <td>95.606280</td>\n",
       "      <td>7.353912</td>\n",
       "      <td>2.778337</td>\n",
       "      <td>0.683823</td>\n",
       "      <td>0.884448</td>\n",
       "      <td>4.453091</td>\n",
       "      <td>6.721507</td>\n",
       "      <td>2.558315</td>\n",
       "      <td>1.800922</td>\n",
       "      <td>10.420990</td>\n",
       "      <td>2.587018</td>\n",
       "      <td>2.433241</td>\n",
       "      <td>32.603198</td>\n",
       "      <td>3.412287</td>\n",
       "      <td>11.024165</td>\n",
       "      <td>65.100051</td>\n",
       "      <td>4.191719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>782.250000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>68.100000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>33.333300</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2073.500000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>73.450000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4376.500000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>78.775000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5066.000000</td>\n",
       "      <td>941.000000</td>\n",
       "      <td>88.300000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>27.670000</td>\n",
       "      <td>39.670000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RID        SITE         AGE    PTEDUCAT       APOE4  \\\n",
       "count   550.000000  550.000000  550.000000  550.000000  550.000000   \n",
       "mean   2385.623636   74.014545   73.052545   16.018182    0.640000   \n",
       "std    1775.001570   95.606280    7.353912    2.778337    0.683823   \n",
       "min       4.000000    2.000000   55.000000    6.000000    0.000000   \n",
       "25%     782.250000   23.000000   68.100000   14.000000    0.000000   \n",
       "50%    2073.500000   57.000000   73.450000   16.000000    1.000000   \n",
       "75%    4376.500000  123.000000   78.775000   18.000000    1.000000   \n",
       "max    5066.000000  941.000000   88.300000   20.000000    2.000000   \n",
       "\n",
       "            CDRSB      ADAS11      ADAS13      ADASQ4        MMSE  \\\n",
       "count  550.000000  549.000000  547.000000  550.000000  550.000000   \n",
       "mean     1.509091   10.198506   16.460037    5.523636   27.594545   \n",
       "std      0.884448    4.453091    6.721507    2.558315    1.800922   \n",
       "min      0.500000    2.000000    3.000000    0.000000   23.000000   \n",
       "25%      1.000000    7.000000   11.000000    4.000000   26.000000   \n",
       "50%      1.500000   10.000000   16.000000    5.000000   28.000000   \n",
       "75%      2.000000   13.000000   21.000000    7.000000   29.000000   \n",
       "max      5.500000   27.670000   39.670000   10.000000   30.000000   \n",
       "\n",
       "       RAVLT_immediate  RAVLT_learning  RAVLT_forgetting  \\\n",
       "count       550.000000      550.000000        550.000000   \n",
       "mean         34.330909        4.092727          4.665455   \n",
       "std          10.420990        2.587018          2.433241   \n",
       "min          11.000000       -1.000000         -2.000000   \n",
       "25%          27.000000        2.000000          3.000000   \n",
       "50%          33.000000        4.000000          5.000000   \n",
       "75%          41.000000        6.000000          6.000000   \n",
       "max          68.000000       11.000000         13.000000   \n",
       "\n",
       "       RAVLT_perc_forgetting   LDELTOTAL   DIGITSCOR    TRABSCOR         FAQ  \n",
       "count             550.000000  550.000000  260.000000  546.000000  546.000000  \n",
       "mean               60.799743    5.676364   37.653846  115.073260    3.214286  \n",
       "std                32.603198    3.412287   11.024165   65.100051    4.191719  \n",
       "min               -25.000000    0.000000    5.000000   33.000000    0.000000  \n",
       "25%                33.333300    3.000000   30.000000   71.000000    0.000000  \n",
       "50%                62.500000    6.000000   38.000000   96.000000    2.000000  \n",
       "75%               100.000000    9.000000   45.000000  128.000000    5.000000  \n",
       "max               100.000000   15.000000   69.000000  300.000000   22.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECODE CATEGORICAL VARIABLES WITH NUMBERS (SKLEARN DOES NOT ACCECPT ANY CATEGORICAL VARIABLE EVEN FOR MODELS WHICH NATIVELY DO IT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORICAL COLUMS ARE [2,4,6,21]\n",
    "\n",
    "# 2 - DX_bl\n",
    "DX_bl = {'EMCI': 0,'LMCI': 1}\n",
    "dataset.DX_bl = [DX_bl[item] for item in dataset.DX_bl]\n",
    "\n",
    "# 4 - PTGENDER\n",
    "PTGENDER = {'Male': 0,'Female': 1}\n",
    "dataset.PTGENDER = [PTGENDER[item] for item in dataset.PTGENDER]\n",
    "\n",
    "# 6 - PTMARRY\n",
    "PTMARRY = {'Never married':0, u'Married': 1, 'Divorced': 2, 'Widowed': 3, 'Unknown':4 }\n",
    "dataset.PTMARRY = [PTMARRY[item] for item in dataset.PTMARRY]\n",
    "dataset.PTMARRY = dataset.PTMARRY.astype(\"int\")\n",
    "\n",
    "# 20 - CONVERSION_AT_3Y\n",
    "CONVERSION_AT_3Y = {'NO': 0,'YES': 1}\n",
    "dataset.CONVERSION_AT_3Y = [CONVERSION_AT_3Y[item] for item in dataset.CONVERSION_AT_3Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFICATION OF VARIABLES WITH TOO MANY MISSING VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# REPLACE \"UNKNOWN\" IN PTMARRY AS MISSING VALUE\n",
    "dataset.PTMARRY[dataset.PTMARRY == 4] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING VALUES IN 'RID' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'SITE' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'DX_bl' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'AGE' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'PTGENDER' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'PTEDUCAT' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'PTMARRY' ARE: 3 - 0.55%\n",
      "MISSING VALUES IN 'APOE4' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'CDRSB' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'ADAS11' ARE: 1 - 0.18%\n",
      "MISSING VALUES IN 'ADAS13' ARE: 3 - 0.55%\n",
      "MISSING VALUES IN 'ADASQ4' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'MMSE' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'RAVLT_immediate' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'RAVLT_learning' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'RAVLT_forgetting' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'RAVLT_perc_forgetting' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'LDELTOTAL' ARE: 0 - 0.0%\n",
      "MISSING VALUES IN 'DIGITSCOR' ARE: 290 - 52.73%\n",
      "---------------------------> MISSING VALUES EXCEEDING THE CHOSEN DROP RATE OF 0.2\n",
      "---------------------------> VARIABLE 'DIGITSCOR' WILL BE REMOVED\n",
      "MISSING VALUES IN 'TRABSCOR' ARE: 4 - 0.73%\n",
      "MISSING VALUES IN 'FAQ' ARE: 4 - 0.73%\n",
      "MISSING VALUES IN 'CONVERSION_AT_3Y' ARE: 0 - 0.0%\n"
     ]
    }
   ],
   "source": [
    "# AUTOMATICALLY REMOVE VARIABLES WITH MORE THAN A CERTAIN RATE OF MISSING VALUES\n",
    "\n",
    "# THE DROP RATE\n",
    "drop_rate = 0.2\n",
    "\n",
    "# LOOP OVER AND IDENTIFY VARIABLES THAT NEED TO BE DROPPED\n",
    "predictor_to_drop = list()\n",
    "for j in range(dataset.shape[1]):\n",
    "    print(\"MISSING VALUES IN '\"+dataset.columns[j]+\"' ARE: \"+str(sum(dataset.iloc[:,j].isnull()))+\" - \"+str(round(sum(dataset.iloc[:,j].isnull())/dataset.shape[0]*100, 2))+\"%\")\n",
    "    \n",
    "    if sum(dataset.iloc[:,j].isnull()) > dataset.shape[0]*drop_rate:\n",
    "        predictor_to_drop.append(dataset.columns[j])\n",
    "        print(\"---------------------------> MISSING VALUES EXCEEDING THE CHOSEN DROP RATE OF \"+str(drop_rate))\n",
    "        print(\"---------------------------> VARIABLE '\"+dataset.columns[j]+\"' WILL BE REMOVED\")\n",
    "\n",
    "# DROP THE VARIABLES FROM THE DATASET\n",
    "dataset = dataset.drop(columns = predictor_to_drop)\n",
    "del predictor_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE PREDICTORS AND OUTCOME ARRAYS, AND SPLIT THEM IN TRAIN AND TEST DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEPING TRACK OF COLUMN NAMES, AS WHEN THE DATASET IS CONVERTED IN np.array THE NAMES ARE LOST\n",
    "# LAST COLUMN IS THE OUTCOME VARIABLE, SO IT IS EXCLUDED. RID (index 0) AND SITE (index 1) ARE JUST ID VARIABLES.\n",
    "\n",
    "predictor_names = dataset.columns[0:(dataset.shape[1]-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of cases hold-out for the test set is:\n",
      "0.8018181818181818\n",
      "____________________________\n",
      "N cases in train set: 441\n",
      "% of CONVERTERS in the train set is: 0.35827664399092973\n",
      "% of NON- CONVERTERS in the train set is: 0.6417233560090703\n",
      "N cases in train set: 109\n",
      "% of CONVERTERS in the train set is:0.3577981651376147\n",
      "% of NON- CONVERTERS in the train set is:0.6422018348623854\n"
     ]
    }
   ],
   "source": [
    "# CREATE PREDICOTRS AND OUTCOME ARRAYS\n",
    "\n",
    "# The outcome variable is CONVERSION_AT_3Y, the last column\n",
    "X = dataset.iloc[:,0:(dataset.shape[1]-1)].values\n",
    "y = dataset[\"CONVERSION_AT_3Y\"].values\n",
    "\n",
    "\n",
    "################################\n",
    "## SITE-INDEPENDENT TEST FOLD ##\n",
    "################################\n",
    "\n",
    "# split in train and test set, holding-out out for testing the subjects recruited in the sites allocated to the current test fold\n",
    "\n",
    "# read the current fold from the current working directory and assing the correct sites for this test fold\n",
    "fold = [int(os.getcwd()[-1])][0]\n",
    "\n",
    "if fold == 1:\n",
    "    SITE_test = [6,12,18,21,126,127,128,137]\n",
    "elif fold == 2:\n",
    "    SITE_test = [2,3,9,23,24,29,36,37,94,99,114]\n",
    "elif fold == 3:\n",
    "    SITE_test = [7,13,14,33,41,67,73,98,100,109,116]\n",
    "elif fold == 4:\n",
    "    SITE_test = [5,16,22,27,31,35,123,130,141,153]\n",
    "elif fold == 5:\n",
    "    SITE_test = [10,11,19,32,51,52,53,57,62,68,72,82,129,131,133,135,136,941]\n",
    "    \n",
    "# CREATE THE TRAIN/TEST SPLIT\n",
    "\n",
    "X_train = X[[i for i in range(X.shape[0]) if X[i,int(np.array(np.where(np.array(predictor_names) == \"SITE\")))] not in SITE_test],:]\n",
    "X_test = X[[i for i in range(X.shape[0]) if X[i,int(np.array(np.where(np.array(predictor_names) == \"SITE\")))] in SITE_test],:]\n",
    "\n",
    "y_train = y[[i for i in range(X.shape[0]) if X[i,int(np.array(np.where(np.array(predictor_names) == \"SITE\")))] not in SITE_test]]\n",
    "y_test = y[[i for i in range(X.shape[0]) if X[i,int(np.array(np.where(np.array(predictor_names) == \"SITE\")))] in SITE_test]]\n",
    "\n",
    "\n",
    "# CHECK IF THE TRAIN AND TEST SET ARE APPROXIMATELY STRATIFIED FOR THE OUTCOME AND APPROXIMATELY 15% OF CASES HAV BEEN ASSIGNET TO THE TEST SET\n",
    "\n",
    "print(\"% of cases hold-out for the test set is:\")\n",
    "print(y_train.shape[0]/y.shape[0])\n",
    "\n",
    "print(\"____________________________\")\n",
    "\n",
    "print(\"N cases in train set: \"+str(y_train.shape[0]))\n",
    "print(\"% of CONVERTERS in the train set is: \"+str(y_train[y_train == 1].shape[0]/y_train.shape[0]))\n",
    "print(\"% of NON- CONVERTERS in the train set is: \"+str(y_train[y_train == 0].shape[0]/y_train.shape[0]))\n",
    "\n",
    "\n",
    "print(\"N cases in train set: \"+str(y_test.shape[0]))\n",
    "print(\"% of CONVERTERS in the train set is:\"+str(y_test[y_test == 1].shape[0]/y_test.shape[0]))\n",
    "print(\"% of NON- CONVERTERS in the train set is:\"+str(y_test[y_test == 0].shape[0]/y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSING VALUES IN THE PREDICTORS OF BOTH TRAIN/TEST DATASET ARE INPUTED WITH THE MEDIAN OF THE TRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputation = np.zeros(X_train.shape[1])\n",
    "for j in range(X_train.shape[1]):\n",
    "    median_imputation[j] = np.nanmedian(X_train[:,j])\n",
    "    X_train[np.where(np.isnan(X_train[:,j]) == True),j] = median_imputation[j]\n",
    "    X_test[np.where(np.isnan(X_test[:,j]) == True),j] = median_imputation[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECODE POLYTOMOUS CATEGORICAL PREDICTOR WITH ONE-HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PTMARRY (index 6)\n",
    "enc = OneHotEncoder(categorical_features=[6], sparse=False)\n",
    "enc.fit(X_train)   \n",
    "\n",
    "X_train = enc.transform(X_train)\n",
    "X_test = enc.transform(X_test)\n",
    "\n",
    "# APOE4 (index 11)\n",
    "enc = OneHotEncoder(categorical_features=[10], sparse=False)\n",
    "enc.fit(X_train)   \n",
    "\n",
    "# TRANFORM THE DATASET\n",
    "X_train = enc.transform(X_train)\n",
    "X_test = enc.transform(X_test)\n",
    "\n",
    "# UPDATE THE PREDICOTOR NAMES LIST\n",
    "\n",
    "# removing 'PTMARRY'and 'APOE4' from the name of predictors using list comprehension\n",
    "predictor_names = [x for x in predictor_names if x!='PTMARRY']\n",
    "predictor_names = [x for x in predictor_names if x!='APOE4']\n",
    "\n",
    "# names of the new one-hot predictors\n",
    "new_predictor = ['APOE4_0','APOE4_1','APOE4_2','PTMARRY_Never married', 'PTMARRY_Married', 'PTMARRY_Divorced', 'PTMARRY_Widowed']\n",
    "\n",
    "# make the new correct list of predictor names\n",
    "predictor_names = new_predictor + predictor_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STANDARDIZE CONTINUOS PREDICTORS (MEAN=0; SD=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINUOS PREDICTORS ARE [[11,14:X_train.shape[1]]\n",
    "features_to_scale = [x for x in [range(14,X_train.shape[1])]]\n",
    "features_to_scale = np.append([11],features_to_scale)\n",
    "\n",
    "# CREATE THE SCALER, TRAINED IN X_train\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[:,features_to_scale])\n",
    "\n",
    "# TRANSFORM BOTH X_traiN AND X_test\n",
    "X_train[:,features_to_scale] = scaler.transform(X_train[:,features_to_scale])\n",
    "X_test[:,features_to_scale] = scaler.transform(X_test[:,features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CORRELATED PREDICTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate if there exists in the train set very correlated features (r >.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APOE4_0</th>\n",
       "      <th>APOE4_1</th>\n",
       "      <th>APOE4_2</th>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <th>PTMARRY_Divorced</th>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <th>RID</th>\n",
       "      <th>SITE</th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>...</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>ADASQ4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>FAQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>APOE4_0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOE4_1</th>\n",
       "      <td>-0.79</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOE4_2</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTMARRY_Never married</th>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTMARRY_Married</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTMARRY_Divorced</th>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTMARRY_Widowed</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RID</th>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SITE</th>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DX_bl</th>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTGENDER</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDRSB</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAS11</th>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAS13</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADASQ4</th>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMSE</th>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.57</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRABSCOR</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAQ</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       APOE4_0  APOE4_1  APOE4_2  PTMARRY_Never married  \\\n",
       "APOE4_0                   1.00    -0.79    -0.34                   0.06   \n",
       "APOE4_1                  -0.79     1.00    -0.31                  -0.03   \n",
       "APOE4_2                  -0.34    -0.31     1.00                  -0.05   \n",
       "PTMARRY_Never married     0.06    -0.03    -0.05                   1.00   \n",
       "PTMARRY_Married          -0.08     0.05     0.05                  -0.23   \n",
       "PTMARRY_Divorced         -0.03     0.02     0.02                  -0.04   \n",
       "PTMARRY_Widowed           0.11    -0.07    -0.06                  -0.05   \n",
       "RID                       0.02    -0.03     0.02                   0.07   \n",
       "SITE                      0.07    -0.02    -0.07                   0.17   \n",
       "DX_bl                    -0.07     0.06     0.03                  -0.10   \n",
       "AGE                       0.07     0.02    -0.14                   0.07   \n",
       "PTGENDER                  0.03    -0.02    -0.02                   0.05   \n",
       "PTEDUCAT                  0.01     0.01    -0.02                   0.07   \n",
       "CDRSB                    -0.17     0.15     0.03                  -0.06   \n",
       "ADAS11                   -0.23     0.16     0.11                  -0.05   \n",
       "ADAS13                   -0.25     0.16     0.14                  -0.04   \n",
       "ADASQ4                   -0.24     0.14     0.15                  -0.02   \n",
       "MMSE                      0.16    -0.09    -0.10                   0.04   \n",
       "RAVLT_immediate           0.15    -0.10    -0.08                  -0.07   \n",
       "RAVLT_learning            0.14    -0.08    -0.11                  -0.02   \n",
       "RAVLT_forgetting         -0.15     0.10     0.07                  -0.05   \n",
       "RAVLT_perc_forgetting    -0.24     0.16     0.12                  -0.02   \n",
       "LDELTOTAL                 0.17    -0.11    -0.09                   0.10   \n",
       "TRABSCOR                 -0.04     0.09    -0.07                   0.00   \n",
       "FAQ                      -0.17     0.17     0.01                  -0.04   \n",
       "\n",
       "                       PTMARRY_Married  PTMARRY_Divorced  PTMARRY_Widowed  \\\n",
       "APOE4_0                          -0.08             -0.03             0.11   \n",
       "APOE4_1                           0.05              0.02            -0.07   \n",
       "APOE4_2                           0.05              0.02            -0.06   \n",
       "PTMARRY_Never married            -0.23             -0.04            -0.05   \n",
       "PTMARRY_Married                   1.00             -0.61            -0.66   \n",
       "PTMARRY_Divorced                 -0.61              1.00            -0.12   \n",
       "PTMARRY_Widowed                  -0.66             -0.12             1.00   \n",
       "RID                              -0.08              0.10            -0.02   \n",
       "SITE                             -0.02             -0.02            -0.02   \n",
       "DX_bl                             0.04             -0.08             0.06   \n",
       "AGE                              -0.14             -0.10             0.25   \n",
       "PTGENDER                         -0.35              0.18             0.28   \n",
       "PTEDUCAT                          0.15              0.03            -0.26   \n",
       "CDRSB                             0.06             -0.03            -0.03   \n",
       "ADAS11                            0.12             -0.15            -0.00   \n",
       "ADAS13                            0.12             -0.17             0.01   \n",
       "ADASQ4                            0.09             -0.13             0.01   \n",
       "MMSE                              0.00              0.01            -0.03   \n",
       "RAVLT_immediate                  -0.09              0.13             0.02   \n",
       "RAVLT_learning                   -0.11              0.10             0.06   \n",
       "RAVLT_forgetting                 -0.02              0.03             0.02   \n",
       "RAVLT_perc_forgetting             0.07             -0.08            -0.01   \n",
       "LDELTOTAL                        -0.05              0.12            -0.08   \n",
       "TRABSCOR                          0.03             -0.16             0.10   \n",
       "FAQ                               0.12             -0.09            -0.06   \n",
       "\n",
       "                        RID  SITE  DX_bl  ...   ADAS13  ADASQ4  MMSE  \\\n",
       "APOE4_0                0.02  0.07  -0.07  ...    -0.25   -0.24  0.16   \n",
       "APOE4_1               -0.03 -0.02   0.06  ...     0.16    0.14 -0.09   \n",
       "APOE4_2                0.02 -0.07   0.03  ...     0.14    0.15 -0.10   \n",
       "PTMARRY_Never married  0.07  0.17  -0.10  ...    -0.04   -0.02  0.04   \n",
       "PTMARRY_Married       -0.08 -0.02   0.04  ...     0.12    0.09  0.00   \n",
       "PTMARRY_Divorced       0.10 -0.02  -0.08  ...    -0.17   -0.13  0.01   \n",
       "PTMARRY_Widowed       -0.02 -0.02   0.06  ...     0.01    0.01 -0.03   \n",
       "RID                    1.00  0.04  -0.49  ...    -0.14   -0.14  0.19   \n",
       "SITE                   0.04  1.00  -0.05  ...    -0.07   -0.09  0.06   \n",
       "DX_bl                 -0.49 -0.05   1.00  ...     0.38    0.38 -0.27   \n",
       "AGE                   -0.20  0.05   0.16  ...     0.19    0.12 -0.21   \n",
       "PTGENDER               0.06  0.02  -0.01  ...    -0.10   -0.06 -0.00   \n",
       "PTEDUCAT               0.08  0.00  -0.05  ...    -0.18   -0.15  0.19   \n",
       "CDRSB                 -0.01 -0.01   0.23  ...     0.31    0.26 -0.22   \n",
       "ADAS11                -0.10 -0.04   0.33  ...     0.94    0.65 -0.39   \n",
       "ADAS13                -0.14 -0.07   0.38  ...     1.00    0.84 -0.44   \n",
       "ADASQ4                -0.14 -0.09   0.38  ...     0.84    1.00 -0.39   \n",
       "MMSE                   0.19  0.06  -0.27  ...    -0.44   -0.39  1.00   \n",
       "RAVLT_immediate        0.19  0.05  -0.36  ...    -0.65   -0.60  0.36   \n",
       "RAVLT_learning         0.17 -0.00  -0.31  ...    -0.51   -0.49  0.29   \n",
       "RAVLT_forgetting       0.03 -0.10   0.08  ...     0.21    0.28 -0.04   \n",
       "RAVLT_perc_forgetting -0.08 -0.10   0.29  ...     0.56    0.57 -0.27   \n",
       "LDELTOTAL              0.34  0.08  -0.71  ...    -0.55   -0.52  0.38   \n",
       "TRABSCOR              -0.12 -0.02   0.19  ...     0.35    0.22 -0.28   \n",
       "FAQ                   -0.09 -0.04   0.23  ...     0.32    0.31 -0.14   \n",
       "\n",
       "                       RAVLT_immediate  RAVLT_learning  RAVLT_forgetting  \\\n",
       "APOE4_0                           0.15            0.14             -0.15   \n",
       "APOE4_1                          -0.10           -0.08              0.10   \n",
       "APOE4_2                          -0.08           -0.11              0.07   \n",
       "PTMARRY_Never married            -0.07           -0.02             -0.05   \n",
       "PTMARRY_Married                  -0.09           -0.11             -0.02   \n",
       "PTMARRY_Divorced                  0.13            0.10              0.03   \n",
       "PTMARRY_Widowed                   0.02            0.06              0.02   \n",
       "RID                               0.19            0.17              0.03   \n",
       "SITE                              0.05           -0.00             -0.10   \n",
       "DX_bl                            -0.36           -0.31              0.08   \n",
       "AGE                              -0.21           -0.07             -0.04   \n",
       "PTGENDER                          0.24            0.12              0.03   \n",
       "PTEDUCAT                          0.21            0.13              0.02   \n",
       "CDRSB                            -0.20           -0.15              0.04   \n",
       "ADAS11                           -0.59           -0.46              0.16   \n",
       "ADAS13                           -0.65           -0.51              0.21   \n",
       "ADASQ4                           -0.60           -0.49              0.28   \n",
       "MMSE                              0.36            0.29             -0.04   \n",
       "RAVLT_immediate                   1.00            0.58             -0.18   \n",
       "RAVLT_learning                    0.58            1.00             -0.02   \n",
       "RAVLT_forgetting                 -0.18           -0.02              1.00   \n",
       "RAVLT_perc_forgetting            -0.61           -0.48              0.78   \n",
       "LDELTOTAL                         0.50            0.44             -0.17   \n",
       "TRABSCOR                         -0.33           -0.18             -0.06   \n",
       "FAQ                              -0.26           -0.28              0.11   \n",
       "\n",
       "                       RAVLT_perc_forgetting  LDELTOTAL  TRABSCOR   FAQ  \n",
       "APOE4_0                                -0.24       0.17     -0.04 -0.17  \n",
       "APOE4_1                                 0.16      -0.11      0.09  0.17  \n",
       "APOE4_2                                 0.12      -0.09     -0.07  0.01  \n",
       "PTMARRY_Never married                  -0.02       0.10      0.00 -0.04  \n",
       "PTMARRY_Married                         0.07      -0.05      0.03  0.12  \n",
       "PTMARRY_Divorced                       -0.08       0.12     -0.16 -0.09  \n",
       "PTMARRY_Widowed                        -0.01      -0.08      0.10 -0.06  \n",
       "RID                                    -0.08       0.34     -0.12 -0.09  \n",
       "SITE                                   -0.10       0.08     -0.02 -0.04  \n",
       "DX_bl                                   0.29      -0.71      0.19  0.23  \n",
       "AGE                                     0.04      -0.12      0.30  0.00  \n",
       "PTGENDER                               -0.06      -0.06     -0.02 -0.10  \n",
       "PTEDUCAT                               -0.09       0.31     -0.24  0.03  \n",
       "CDRSB                                   0.20      -0.28      0.17  0.53  \n",
       "ADAS11                                  0.48      -0.49      0.32  0.28  \n",
       "ADAS13                                  0.56      -0.55      0.35  0.32  \n",
       "ADASQ4                                  0.57      -0.52      0.22  0.31  \n",
       "MMSE                                   -0.27       0.38     -0.28 -0.14  \n",
       "RAVLT_immediate                        -0.61       0.50     -0.33 -0.26  \n",
       "RAVLT_learning                         -0.48       0.44     -0.18 -0.28  \n",
       "RAVLT_forgetting                        0.78      -0.17     -0.06  0.11  \n",
       "RAVLT_perc_forgetting                   1.00      -0.46      0.13  0.28  \n",
       "LDELTOTAL                              -0.46       1.00     -0.23 -0.29  \n",
       "TRABSCOR                                0.13      -0.23      1.00  0.16  \n",
       "FAQ                                     0.28      -0.29      0.16  1.00  \n",
       "\n",
       "[25 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  1,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "        14, 15, 15, 15, 16, 16, 17, 18, 19, 20, 20, 21, 21, 22, 23, 24]),\n",
       " array([ 0,  1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "        15, 14, 15, 16, 15, 16, 17, 18, 19, 20, 21, 20, 21, 22, 23, 24]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define function to calculate the correlation matrix\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def corrcoef_loop(matrix):\n",
    "    rows, cols = matrix.shape[0], matrix.shape[1]\n",
    "    r = np.ones(shape=(rows, rows))\n",
    "    p = np.ones(shape=(rows, rows))\n",
    "    for i in range(rows):\n",
    "        for j in range(i+1, rows):\n",
    "            r_, p_ = pearsonr(matrix[i], matrix[j])\n",
    "            r[i, j] = r[j, i] = r_\n",
    "            p[i, j] = p[j, i] = p_\n",
    "    return r, p\n",
    "\n",
    "# Calculate and print the correlation matrix for the training dataset\n",
    "\n",
    "matrix = corrcoef_loop(np.transpose(X_train))\n",
    "matrix = pd.DataFrame(np.around(matrix[0],2))\n",
    "matrix.index = predictor_names\n",
    "matrix.columns = predictor_names\n",
    "display(matrix)\n",
    "\n",
    "# Highlight couples of predictors above the define threshold\n",
    "\n",
    "display(np.where(np.abs(matrix) > 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate components and substitute them in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the components of the first group of predictors\n",
      "[ 2.63507959  0.35770888  0.01402971]\n",
      "Number of components with explained variance > 1.0: 1\n",
      " \n",
      "Explained variance of the components of the second group of predictors\n",
      "[ 1.78663571  0.21790974]\n",
      "Number of components with explained variance > 1.0: 1\n"
     ]
    }
   ],
   "source": [
    "# Fit the PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCA_corr_1 = PCA(copy=False)\n",
    "PCA_corr_2 = PCA(copy=False)\n",
    "\n",
    "PCA_corr_1.fit(X_train[:,[14,15,16]])\n",
    "PCA_corr_2.fit(X_train[:,[20,21]])\n",
    "\n",
    "print(\"Explained variance of the components of the first group of predictors\")\n",
    "print(PCA_corr_1.explained_variance_)\n",
    "print(\"Number of components with explained variance > 1.0: \"+ str(len(np.where(PCA_corr_1.explained_variance_ > 1))))\n",
    "print(\" \")\n",
    "print(\"Explained variance of the components of the second group of predictors\")\n",
    "print(PCA_corr_2.explained_variance_)\n",
    "print(\"Number of components with explained variance > 1.0: \"+ str(len(np.where(PCA_corr_2.explained_variance_ > 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the components\n",
    "\n",
    "f_1_train = PCA_corr_1.transform(X_train[:,[14,15,16]])[:,0]\n",
    "f_2_train = PCA_corr_2.transform(X_train[:,[20,21]])[:,0]\n",
    "\n",
    "f_1_test = PCA_corr_1.transform(X_test[:,[14,15,16]])[:,0]\n",
    "f_2_test = PCA_corr_2.transform(X_test[:,[20,21]])[:,0]\n",
    "\n",
    "f_1_train = f_1_train.reshape((f_1_train.shape[0],1))\n",
    "f_2_train = f_2_train.reshape((f_2_train.shape[0],1))\n",
    "\n",
    "f_1_test = f_1_test.reshape((f_1_test.shape[0],1))\n",
    "f_2_test = f_2_test.reshape((f_2_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the correlated predicotrs in the datasets with their first PCs\n",
    "\n",
    "features_to_keep = [i for i in range(X_train.shape[1]) if i not in [14,15,16,20,21]]\n",
    "\n",
    "X_train = np.append(X_train[:,features_to_keep],f_1_train, axis=1)\n",
    "X_train = np.append(X_train,f_2_train, axis=1)\n",
    "\n",
    "X_test = np.append(X_test[:,features_to_keep],f_1_test, axis=1)\n",
    "X_test = np.append(X_test,f_2_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the predictor_names vector\n",
    "\n",
    "predictor_names = [predictor_names[i] for i in features_to_keep]\n",
    "predictor_names = np.append(np.append(predictor_names,\"PCA_1_ADAS\"),\"PCA_1_RAVLT_forgetting\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE_SET_1: ALL FEATURES - EXCLUDING APOE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PTMARRY_Never married', 'PTMARRY_Married', 'PTMARRY_Divorced', 'PTMARRY_Widowed', 'DX_bl', 'AGE', 'PTGENDER', 'PTEDUCAT', 'CDRSB', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning', 'LDELTOTAL', 'TRABSCOR', 'FAQ', 'PCA_1_ADAS', 'PCA_1_RAVLT_forgetting']\n"
     ]
    }
   ],
   "source": [
    "# ID ['RID', 'SITE'] + APOE4 ['APOE4_0', 'APOE4_1', 'APOE4_2'] FEATURES ARE EXCLUDED\n",
    "\n",
    "# CREATE AN ARRAY WITH INDEXES OF FEATURES TO BE USED IN THE CURRENT FEATURE SET\n",
    "feature_set_1 = [list(range(X_train.shape[1]))[i] for i in range(X_train.shape[1]) if predictor_names[i] not in ['RID', 'SITE','APOE4_0', 'APOE4_1', 'APOE4_2']]\n",
    "print([predictor_names[i] for i in feature_set_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE_SET_2: FILTERING P<0.05 - EXCLUDING APOE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DX_bl', 'AGE', 'CDRSB', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning', 'LDELTOTAL', 'TRABSCOR', 'FAQ', 'PCA_1_ADAS', 'PCA_1_RAVLT_forgetting']\n"
     ]
    }
   ],
   "source": [
    "# CREATE AND FIT THE FILTER\n",
    "selector_feature_set_2 = SelectFwe(alpha=0.05)\n",
    "selector_feature_set_2.fit(X_train, y=y_train)\n",
    "\n",
    "# CREATE AN ARRAY WITH INDEXES OF FEATURES TO BE USED IN THE CURRENT FEATURE SET ACCORDING TO THE SELECTION STRATEGY\n",
    "feature_set_2 = [list(range(X_train.shape[1]))[i] for i in range(X_train.shape[1]) if selector_feature_set_2.pvalues_[i] < 0.05 and predictor_names[i] not in ['RID', 'SITE','APOE4_0', 'APOE4_1', 'APOE4_2']]\n",
    "print([predictor_names[i] for i in feature_set_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE_SET_5: WRAPPER (RFE) WITH LOGISTIC REGRESSION - EXCLUDING APOE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PTMARRY_Never married', 'PTMARRY_Married', 'PTMARRY_Divorced', 'PTMARRY_Widowed', 'DX_bl', 'PTGENDER', 'CDRSB', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning', 'LDELTOTAL', 'TRABSCOR', 'FAQ', 'PCA_1_ADAS', 'PCA_1_RAVLT_forgetting']\n"
     ]
    }
   ],
   "source": [
    "# DEFINE CROSS-VALIDATION ITERATOR (THE SAME AS IT WILL BE DEFINED LATER DURING THE TRAINING THE MODELS)\n",
    "n_fold = 10\n",
    "n_repeats = 10\n",
    "cv_random_seed = 1\n",
    "rkf = RepeatedStratifiedKFold(n_splits=n_fold, n_repeats=n_repeats, random_state=cv_random_seed)\n",
    "\n",
    "# CREATE AND FIT THE FILTER\n",
    "selector_feature_set_5 = RFECV(estimator=LogisticRegression(penalty = 'l2', C = sys.maxsize), \\\n",
    "                             cv=rkf, \\\n",
    "                             scoring = \"roc_auc\",\\\n",
    "                             n_jobs = -1)\n",
    "selector_feature_set_5.fit(X_train[:,feature_set_1], y=y_train)\n",
    "\n",
    "# CREATE AN ARRAY WITH INDEXES OF FEATURES TO BE USED IN THE CURRENT FEATURE SET ACCORDING TO THE SELECTION STRATEGY\n",
    "feature_set_5 = [feature_set_1[i] for i in range(len(selector_feature_set_5.support_)) if selector_feature_set_5.support_[i] == True ]\n",
    "print([predictor_names[i] for i in feature_set_5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE_SET_7: WRAPPER (RFE) WITH RANDOM FOREST WITH DEFAULT HYPERPARAMETERS - EXCLUDING APOE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DX_bl', 'AGE', 'PTGENDER', 'PTEDUCAT', 'CDRSB', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning', 'LDELTOTAL', 'TRABSCOR', 'FAQ', 'PCA_1_ADAS', 'PCA_1_RAVLT_forgetting']\n"
     ]
    }
   ],
   "source": [
    "# LOAD PICKLED FEATURE_SET_7, IF EXISTS\n",
    "\n",
    "for file in listdir(getcwd()+\"/files\"):\n",
    "    if file.endswith(\"_feature_set_7.csv\"):\n",
    "        selector_feature_set_7 = pd.read_csv(\"files/\"+file)\n",
    "        \n",
    "# SET HOW MANY REPETITION OF THE RFE \n",
    "\n",
    "rep = 10\n",
    "\n",
    "if \"selector_feature_set_7\" not in globals():\n",
    "\n",
    "    selector_feature_set_7 = pd.DataFrame(np.zeros((rep,len(feature_set_1))))\n",
    "    \n",
    "    for i in range(rep):\n",
    "        \n",
    "        # SET SEED\n",
    "        np.random.seed(i)\n",
    "    \n",
    "        # CREATE AND FIT THE FILTER\n",
    "        selector_feature_set_this = RFECV(estimator= sklearn.ensemble.RandomForestClassifier(n_estimators=2000), \\\n",
    "                                 cv=rkf, \\\n",
    "                                 scoring = \"roc_auc\",\\\n",
    "                                 n_jobs = -1)\n",
    "        selector_feature_set_this.fit(X_train[:,feature_set_1], y=y_train)\n",
    "        \n",
    "        # FIND INDEX OF SELECTED FEATURES\n",
    "        feature_this = [i for i in range(len(selector_feature_set_this.support_)) if selector_feature_set_this.support_[i] == True ]\n",
    "        \n",
    "        # ADD IT IN THE DATAFRAME\n",
    "        selector_feature_set_7.iloc[i,feature_this] = 1\n",
    "    \n",
    "    # SAVE IT\n",
    "    selector_feature_set_7.to_csv(\"selector_feature_set_7.csv\", index = False)\n",
    "\n",
    "# IDENTIFY WHICH FEATURES WERE SELECTED AT LEAST IN HALF OF THE REPETITIONS\n",
    "pass_the_threshold = selector_feature_set_7.apply(sum) >= (rep/2)\n",
    "\n",
    "# CREATE AN ARRAY WITH INDEXES OF FEATURES TO BE USED IN THE CURRENT FEATURE SET ACCORDING TO THE SELECTION STRATEGY\n",
    "feature_set_7 = [feature_set_1[i] for i in range(selector_feature_set_7.shape[1]) if pass_the_threshold[i] == True ]\n",
    "print([predictor_names[i] for i in feature_set_7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN THE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define log-uniform distribution for random search of svm rbf parameter search\n",
    "class log_uniform():        \n",
    "    def __init__(self, a=-1, b=0, base=10):\n",
    "        self.loc = a\n",
    "        self.scale = b - a\n",
    "        self.base = base\n",
    "\n",
    "    def rvs(self, size=None, random_state=None):\n",
    "        uniform = sp.stats.uniform(loc=self.loc, scale=self.scale)\n",
    "        if size is None:\n",
    "            return np.power(self.base, uniform.rvs(random_state=random_state))\n",
    "        else:\n",
    "             return np.power(self.base, uniform.rvs(size=size, random_state=random_state))\n",
    "            \n",
    "# define the function that will be used to identify the threshold that provides the best balanced accuracy in the train dataset, with cv predictions \n",
    "def calculate_best_threshold_train_dataset(prediction_train_calibration, y_train_calibration = y_train):\n",
    "\n",
    "    index_converters = np.where(y_train_calibration ==1)\n",
    "    index_non_converters = np.where(y_train_calibration ==0)\n",
    "\n",
    "    best_threshold = -10\n",
    "    best_balanced_accuracy = -10\n",
    "\n",
    "    for t in np.arange(0,1.0001,0.0001):\n",
    "        sens = sum(prediction_train_calibration[index_converters] >= t)/prediction_train_calibration[index_converters].shape[0]\n",
    "        spec = sum(prediction_train_calibration[index_non_converters] < t)/prediction_train_calibration[index_non_converters].shape[0]\n",
    "    \n",
    "        if best_balanced_accuracy <= ((sens+spec)/2):\n",
    "            best_threshold = t\n",
    "            best_balanced_accuracy = ((sens+spec)/2)\n",
    "            \n",
    "            # print(t)\n",
    "            # print(best_balanced_accuracy)\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "# define the function that will be used to identify the thresholds that provide desired sensitivities in the train dataset, with cv predictions      \n",
    "def calculate_parametrized_sensitivity_threshold_train_dataset(prediction_train_calibration, y_train_calibration = y_train, desired_sensitivity_levels = [0.95,0.9,0.85,0.8,0.75]):\n",
    "\n",
    "    index_converters = np.where(y_train_calibration ==1)\n",
    "    index_non_converters = np.where(y_train_calibration ==0)\n",
    "\n",
    "    # best_threshold = -10\n",
    "    # current_sensitivity_level = -10\n",
    "    \n",
    "    results_all = pd.DataFrame({\"Threshold\" : [], \"Sensitivity\":[], \"Specificity\":[]})\n",
    "    results_final = pd.DataFrame({\"Sensitivity_theoretical\":desired_sensitivity_levels,\"Threshold\" : np.repeat(-10, len(desired_sensitivity_levels)), \"Sensitivity\":np.repeat(-10, len(desired_sensitivity_levels)), \"Specificity\":np.repeat(-10, len(desired_sensitivity_levels))})\n",
    "    \n",
    "    for t in np.arange(0,1.0001,0.0001):\n",
    "        results_all = results_all.append(pd.Series({\"Threshold\":t,  \\\n",
    "                                                      \"Sensitivity\":(sum(prediction_train_calibration[index_converters] >= t)/prediction_train_calibration[index_converters].shape[0]), \\\n",
    "                                                      \"Specificity\":(sum(prediction_train_calibration[index_non_converters] < t)/prediction_train_calibration[index_non_converters].shape[0] ) \\\n",
    "                                                     }),\\\n",
    "                                           ignore_index=True)\n",
    "        \n",
    "    for i in desired_sensitivity_levels:\n",
    "        index = np.where(  abs(i- results_all[\"Sensitivity\"].values) == np.min(abs(i- results_all[\"Sensitivity\"].values))   )[0]\n",
    "        index = index[len(index)-1]\n",
    "        results_final.loc[results_final.Sensitivity_theoretical == i, [\"Threshold\",\"Sensitivity\",\"Specificity\"]]  = results_all.iloc[index].values\n",
    "\n",
    "    return results_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE CROSS-VALIDATION ITERATION PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEATED STRATIFIED 10-FOLD CV, REPEATED 10 TIMES.\n",
    "n_fold = 10\n",
    "n_repeats = 10\n",
    "\n",
    "# SET SEED FOR REPRODUCIBILITY\n",
    "cv_random_seed = 1\n",
    "\n",
    "# DEFINE CV OBJECT\n",
    "rkf = RepeatedStratifiedKFold(n_splits=n_fold, n_repeats=n_repeats, random_state=cv_random_seed)\n",
    "\n",
    "# DEFINE THE NUMBER OF RANDOM SEARCHES TO PERFORM BEFORE BAYESIAN OPTIMIZATION SEARCH STARTS\n",
    "n_random_combination = 50\n",
    "\n",
    "# DEFINE THE TOTAL NUMBER OF THE BAYESIAN OPTIMIZATION SEARCH, INCLUDING THE INITIAL RANDOM SEARCHES\n",
    "n_BO_combination = 50 + n_random_combination\n",
    "\n",
    "# DEFINE AUROC AS THE METRIC TO BE OPTIMIZED IN THE HYPER-PARAMETER SEARCH\n",
    "scorer_1 = make_scorer(roc_auc_score, needs_proba=True)\n",
    "scorer_this = \"roc_auc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE WHICH MODELS YOU WANT TO TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names\n",
    "\n",
    "models = [\"EN_CATEGORICAL\",\\\n",
    "          \"svm_linear\",\\\n",
    "          \"svm_radial\",\\\n",
    "          \"svm_poly\",\\\n",
    "          \"knn\",\\\n",
    "          \"rf\",\\\n",
    "          \"MLP_1hiddenlayer_batch\",\\\n",
    "          \"MLP_2hiddenlayer_batch\",\\\n",
    "          \"MLP_1hiddenlayer_adam\",\\\n",
    "          \"MLP_2hiddenlayer_adam\",\\\n",
    "          \"gb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create untrained models\n",
    "EN_CATEGORICAL = SGDClassifier(loss= \"log\", penalty = \"elasticnet\") # no way to use the submodel trick with elasticnet for classification in sklearn...\n",
    "svm_linear= sklearn.svm.SVC(kernel='linear', random_state=cv_random_seed, probability=True, max_iter=500)\n",
    "svm_radial= sklearn.svm.SVC(kernel='rbf', random_state=cv_random_seed, probability=True, max_iter=500)\n",
    "svm_poly= sklearn.svm.SVC(kernel='poly', random_state=cv_random_seed, probability=True, max_iter=500)\n",
    "knn = sklearn.neighbors.KNeighborsClassifier()\n",
    "rf = sklearn.ensemble.RandomForestClassifier(n_estimators=2000)\n",
    "gb = sklearn.ensemble.GradientBoostingClassifier(n_estimators=500)\n",
    "\n",
    "# MLP classifiers need to be defined with a wrapper, due to the way hyperparameters are declaired in the class (which is not natively supported by the scikit-optimize API)\n",
    "\n",
    "class MLP_1hiddenlayer_batch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layer1=10, activation=\"logistic\", learning_rate_init = 0.01):\n",
    "        self.layer1 = layer1\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=[self.layer1],\\\n",
    "            activation = self.activation,\\\n",
    "            solver = \"adam\", \\\n",
    "            learning_rate_init = self.learning_rate_init,\\\n",
    "            batch_size = 100000000,\\\n",
    "            max_iter = 2000,\\\n",
    "            random_state = cv_random_seed,\\\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "class MLP_2hiddenlayer_batch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layer1=10, layer2=10, activation=\"logistic\", learning_rate_init = 0.01):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=[self.layer1, self.layer2],\\\n",
    "            activation = self.activation,\\\n",
    "            solver = \"adam\", \\\n",
    "            learning_rate_init = self.learning_rate_init,\\\n",
    "            batch_size = 100000000,\\\n",
    "            max_iter = 2000,\\\n",
    "            random_state = cv_random_seed,\\\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "class MLP_1hiddenlayer_adam(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layer1=10, activation=\"logistic\", learning_rate_init = 0.01, batch_size = 100):\n",
    "        self.layer1 = layer1\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.batch_size = batch_size\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=[self.layer1],\\\n",
    "            activation = self.activation,\\\n",
    "            solver = \"adam\", \\\n",
    "            learning_rate_init = self.learning_rate_init,\\\n",
    "            batch_size = self.batch_size,\\\n",
    "            max_iter = 2000,\\\n",
    "            random_state = cv_random_seed,\\\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)\n",
    "    \n",
    "\n",
    "class MLP_2hiddenlayer_adam(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, layer1=10, layer2=10, activation=\"logistic\", learning_rate_init = 0.01, batch_size = 100):\n",
    "        self.layer1 = layer1\n",
    "        self.layer2 = layer2\n",
    "        self.activation = activation\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.batch_size = batch_size\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=[self.layer1, self.layer2],\\\n",
    "            activation = self.activation,\\\n",
    "            solver = \"adam\", \\\n",
    "            learning_rate_init = self.learning_rate_init,\\\n",
    "            batch_size = self.batch_size,\\\n",
    "            max_iter = 2000,\\\n",
    "            random_state = cv_random_seed,\\\n",
    "            verbose = 0\n",
    "        )\n",
    "        \n",
    "        model.fit(X, y)\n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search for each model\n",
    "\n",
    "def dict_parameter_BO_EN_CATEGORICAL(features_this):\n",
    "    return {'l1_ratio' : (1e-256, 1, 'uniform'),\\\n",
    "            'alpha' : (1e-6, 1e+6, 'log-uniform')}\n",
    "\n",
    "def dict_parameter_BO_svm_linear(features_this):\n",
    "    return {'C': (1e-3, 1e+3, 'log-uniform')}\n",
    "\n",
    "def dict_parameter_BO_svm_radial(features_this):\n",
    "    return {'C' : (1e-3, 1e+3, 'log-uniform'),  \\\n",
    "            'gamma': (1e-3, 1e+3, 'log-uniform')}\n",
    "\n",
    "def dict_parameter_BO_svm_poly(features_this):\n",
    "    return {'degree': (1,3),\\\n",
    "            'coef0': (-1e3, 1e3, 'uniform'),\\\n",
    "            'gamma': (1e-3, 1e+3, 'log-uniform')}\n",
    "\n",
    "def dict_parameter_BO_knn(features_this):\n",
    "    return {\"n_neighbors\" : (1,len(features_this)), \\\n",
    "           \"weights\" : [\"uniform\",\"distance\"],\n",
    "           \"algorithm\" : [\"brute\"],\n",
    "           \"p\": (1, 4, 'uniform')}\n",
    "\n",
    "def dict_parameter_BO_rf(features_this):\n",
    "    return {\"criterion\" : [\"gini\", \"entropy\"],\\\n",
    "                \"max_features\" : (1, len(features_this))}\n",
    "\n",
    "def dict_parameter_BO_MLP_1hiddenlayer_batch(features_this):\n",
    "    return {'learning_rate_init' : (1e-6, 0.5, 'uniform'),\\\n",
    "            'activation': ['logistic', 'relu', 'tanh'],\\\n",
    "            'layer1' : (1, 100)}\n",
    "              \n",
    "def dict_parameter_BO_MLP_2hiddenlayer_batch(features_this):\n",
    "    return {'layer1' : (1, 100),  \\\n",
    "            'layer2' : (1, 100),  \\\n",
    "            'activation': ['logistic', 'relu', 'tanh'], \\\n",
    "            'learning_rate_init' : (1e-6, 0.5, 'uniform')}\n",
    "\n",
    "def dict_parameter_BO_MLP_1hiddenlayer_adam(features_this):\n",
    "    return {'learning_rate_init' : (1e-6, 0.5, 'uniform'),\\\n",
    "            'activation': ['logistic', 'relu', 'tanh'],\\\n",
    "            'layer1' : (1, 100),\\\n",
    "            'batch_size' : (50,round(X_train.shape[0],0))}\n",
    "              \n",
    "def dict_parameter_BO_MLP_2hiddenlayer_adam(features_this):\n",
    "    return {'layer1' : (1, 100),  \\\n",
    "            'layer2' : (1, 100),  \\\n",
    "            'activation': ['logistic', 'relu', 'tanh'], \\\n",
    "            'learning_rate_init' : (1e-6, 0.5, 'uniform'),\\\n",
    "            'batch_size' : (50,round(X_train.shape[0],0))}\n",
    "\n",
    "def dict_parameter_BO_gb(features_this):\n",
    "    return {'learning_rate' : (1e-3, 0.3, 'uniform'),\\\n",
    "            'max_depth' : (2, 6),\\\n",
    "            'min_samples_split' : (2, 4),\\\n",
    "            'subsample' : (0.8, 1, 'uniform'),\\\n",
    "            'max_features' : (None, \"sqrt\")}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODELS THAT HAVE ALREADY BEEN TRAINED AND PICKLED. \n",
    "### FOR THOSE, DON'T RUN AGAIN THE CODE THAT WOULD RE-TRAIN THEM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODELS WHICH HAVE ALREADY BEEN TRAINED AND PICKLED\n",
    "\n",
    "for file in listdir(getcwd()+\"/trained_models\"):\n",
    "    if file.endswith(\".pickle\") and \"_result_\" in file:\n",
    "        name_object = re.sub(\".pickle\",\"\",file)\n",
    "        globals()[name_object] = joblib.load(\"trained_models/\"+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN ALL MODELS, EXCLUDING LOGISTIC REGRESSION AND NAIVE-BAYES CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST IT WILL CHECK IF A CERTAIN MODELS ALREADY EXISTS. ONLY IF NOT, IT WILL TRAIN IT\n",
    "\n",
    "for model in models:\n",
    "    for feature_set in [\"1\",\"2\",\"5\",\"7\"]:\n",
    "        for search in [\"BO\"]:\n",
    "            if search+\"_result_\"+model+\"_feature_set_\"+feature_set not in globals():\n",
    "                \n",
    "                print(\"                              \")\n",
    "                print(\"                              \")\n",
    "                print(\"-------------------------------------------------------\")\n",
    "                print(\"-------------------------------------------------------\")\n",
    "                print(search+\"_result_\"+model+\"_feature_set_\"+feature_set)\n",
    "                print(\"                              \")\n",
    "                \n",
    "                # CREATE MODEL\n",
    "                model_this = globals()[model]\n",
    "                features_this = globals()[\"feature_set_\"+feature_set]\n",
    "                \n",
    "                if search == \"BO\":\n",
    "                    \n",
    "                    if  model == \"EN_CATEGORICAL\" or \"MLP\" in model:\n",
    "                        # as there will be some covergence warnings for very small level of alpha, let's kill them for now\n",
    "                        # also I used a trick to make the MLP in normal full batch training, which results in a warning message \n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                    \n",
    "                    # MLP models have been cretaed with a wrapper, so the need to be called slightly differently \n",
    "                    if \"MLP\" in model:\n",
    "                    \n",
    "                        search_this = BayesSearchCV(estimator=model_this(),\\\n",
    "                                                search_spaces = globals()[\"dict_parameter_\"+search+\"_\"+model](features_this),\\\n",
    "                                                optimizer_kwargs = {\"base_estimator\" : \"GP\", \\\n",
    "                                                                    \"acq_func\" : \"gp_hedge\",\\\n",
    "                                                                    \"n_initial_points\" : n_random_combination,\\\n",
    "                                                                    \"random_state\" : cv_random_seed},\\\n",
    "                                                cv = rkf,\\\n",
    "                                                verbose=1,\\\n",
    "                                                n_iter=n_BO_combination,\\\n",
    "                                                n_jobs=-1,\\\n",
    "                                                scoring= scorer_this,\\\n",
    "                                                random_state=cv_random_seed,\\\n",
    "                                                iid=False,\\\n",
    "                                                return_train_score=True)\n",
    "                        \n",
    "                    else:\n",
    "                        search_this = BayesSearchCV(estimator=model_this,\\\n",
    "                                                search_spaces = globals()[\"dict_parameter_\"+search+\"_\"+model](features_this),\\\n",
    "                                                optimizer_kwargs = {\"base_estimator\" : \"GP\", \\\n",
    "                                                                    \"acq_func\" : \"gp_hedge\",\\\n",
    "                                                                    \"n_initial_points\" : n_random_combination,\\\n",
    "                                                                    \"random_state\" : cv_random_seed},\\\n",
    "                                                cv = rkf,\\\n",
    "                                                verbose=0,\\\n",
    "                                                n_iter=n_BO_combination,\\\n",
    "                                                n_jobs=-1,\\\n",
    "                                                scoring= scorer_this,\\\n",
    "                                                random_state=cv_random_seed,\\\n",
    "                                                iid=False,\\\n",
    "                                                return_train_score=True)\n",
    "                    \n",
    "                    \n",
    "                    def on_step(optim_result):\n",
    "                        print(\" \")\n",
    "                        print(\"TRAIN N° \"+str(len(search_this.cv_results_['mean_fit_time'])))\n",
    "                        print(\"BEST SCORE: %s\" % str(search_this.best_score_))\n",
    "                        print(\" \")\n",
    "                        print(\"best hyperparameters: %s\" % str(search_this.best_params_))\n",
    "                        print(\" \")\n",
    "                        print(\"current hyperparameters: %s\" % search_this.cv_results_['params'][-1])\n",
    "                        print(\" \")\n",
    "                        print(\"-------------------------------------------------------\")\n",
    "                        if search_this.best_score_ >= 1:\n",
    "                            print('Interrupting!')\n",
    "                            \n",
    "                    # \n",
    "                    search_this.fit(X=X_train[:,features_this], y=y_train, callback=on_step)\n",
    "                        \n",
    "                    if model == \"EN_CATEGORICAL\" or \"MLP\" in model:\n",
    "                        # bring back to the default print of warning messages\n",
    "                        warnings.filterwarnings(\"default\")\n",
    "                        \n",
    "                else:\n",
    "                    print(\"Error: you have requested an hyperparameter search that you didn't properly defined\")\n",
    "                    \n",
    "                globals()[search+\"_result_\"+model+\"_feature_set_\"+feature_set] = search_this\n",
    "                \n",
    "                clear_output()\n",
    "                \n",
    "                # PRINT THE BEST CROSS-VALIDATED PERFORMANCE\n",
    "                print(\" \")\n",
    "                print(\"The model \"+search+\"_\"+model+\"_feature_set_\"+feature_set+\" has been trained.\")\n",
    "                print(\" \")\n",
    "                print(\"BEST CV PERFORMANCE: \"+ str(search_this.best_score_))\n",
    "\n",
    "                # DUMP THE MODEL\n",
    "                joblib.dump(search_this, \"trained_models/\"+search+\"_result_\"+model+\"_feature_set_\"+feature_set+\".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN LOGISTIC REGRESSION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST IT WILL CHECK IF A CERTAIN MODELS ALREADY EXISTS. ONLY IF NOT, IT WILL TRAIN IT\n",
    "\n",
    "for feature_set in [\"1\",\"2\",\"5\",\"7\"]:\n",
    "        \n",
    "            if \"LOGISTIC_result_feature_set_\"+feature_set not in globals():\n",
    "                \n",
    "                print(\"                              \")\n",
    "                print(\"                              \")\n",
    "                print(\"-------------------------\")\n",
    "                print(\"-------------------------\")\n",
    "                print(\"LOGISTIC_result_feature_set_\"+feature_set)\n",
    "                print(\"                              \")\n",
    "                \n",
    "                \n",
    "                # CREATE MODEL\n",
    "                features_this = globals()[\"feature_set_\"+feature_set]\n",
    "                \n",
    "                # NB: A PURE UNREGULARIZED LOGISTIC REGRESSION IS NOT IMPLEMENTED IN SKLEARN !?!. I ARTIFICIALLY MADE IT USING AN \"ALMOST INFINITE\" VALUE FOR C\n",
    "                search_this =  GridSearchCV(estimator=LogisticRegression(),\\\n",
    "                                            param_grid = dict(penalty = ['l2'], C = [sys.maxsize]),\\\n",
    "                                            cv=rkf,\\\n",
    "                                            verbose = 1,\\\n",
    "                                            n_jobs=-1,\\\n",
    "                                            scoring = scorer_this,\\\n",
    "                                            iid=False,\\\n",
    "                                            return_train_score=True)\n",
    "                search_this.fit(X=X_train[:,features_this], y=y_train)\n",
    "                \n",
    "                globals()[\"LOGISTIC_result_feature_set_\"+feature_set] = search_this\n",
    "                \n",
    "                # PRINT THE BEST CROSS-VALIDATED PERFORMANCE\n",
    "                print(\" \")\n",
    "                print(\"The model LOGISTIC_feature_set_\"+feature_set+\" has been trained.\")\n",
    "                print(\" \")\n",
    "                print(\"BEST CV PERFORMANCE: \"+ str(search_this.best_score_))\n",
    "\n",
    "                # DUMP THE MODEL\n",
    "                joblib.dump(search_this, \"trained_models/LOGISTIC_result_feature_set_\"+feature_set+\".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN NAIVE-BAYES MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST IT WILL CHECK IF A CERTAIN MODELS ALREADY EXISTS. ONLY IF NOT, IT WILL TRAIN IT\n",
    "\n",
    "for feature_set in [\"1\",\"2\",\"5\",\"7\"]:\n",
    "        \n",
    "            if \"NB_result_feature_set_\"+feature_set not in globals():\n",
    "                \n",
    "                print(\"                              \")\n",
    "                print(\"                              \")\n",
    "                print(\"-------------------------\")\n",
    "                print(\"-------------------------\")\n",
    "                print(\"NB_result_feature_set_\"+feature_set)\n",
    "                print(\"                              \")\n",
    "                \n",
    "                \n",
    "                # CREATE MODEL\n",
    "                features_this = globals()[\"feature_set_\"+feature_set]\n",
    "                \n",
    "                # NB: A PURE UNREGULARIZED LOGISTIC REGRESSION IS NOT IMPLEMENTED IN SKLEARN !?!. I ARTIFICIALLY MADE IT USING AN \"ALMOST INFINITE\" VALUE FOR C\n",
    "                search_this =  GridSearchCV(estimator=GaussianNB(),\\\n",
    "                                            param_grid = dict(priors = [None]),\\\n",
    "                                            cv=rkf,\\\n",
    "                                            verbose = 1,\\\n",
    "                                            n_jobs=-1,\\\n",
    "                                            scoring = scorer_this,\\\n",
    "                                            iid=False,\\\n",
    "                                            return_train_score=True)\n",
    "                search_this.fit(X=X_train[:,features_this], y=y_train)\n",
    "                \n",
    "                globals()[\"NB_result_feature_set_\"+feature_set] = search_this\n",
    "                \n",
    "                # PRINT THE BEST CROSS-VALIDATED PERFORMANCE\n",
    "                print(\" \")\n",
    "                print(\"The model NB_feature_set_\"+feature_set+\" has been trained.\")\n",
    "                print(\" \")\n",
    "                print(\"BEST CV PERFORMANCE: \"+ str(search_this.best_score_))\n",
    "\n",
    "                # DUMP THE MODEL\n",
    "                joblib.dump(search_this, \"trained_models/NB_result_feature_set_\"+feature_set+\".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS-VALIDATED RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODEL</th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_adam_feature_set_7</td>\n",
       "      <td>0.901503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_adam_feature_set_7</td>\n",
       "      <td>0.898309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_adam_feature_set_1</td>\n",
       "      <td>0.897762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_batch_feature_set_1</td>\n",
       "      <td>0.894108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BO_result_gb_feature_set_5</td>\n",
       "      <td>0.893861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_adam_feature_set_2</td>\n",
       "      <td>0.893828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BO_result_rf_feature_set_7</td>\n",
       "      <td>0.892566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_batch_feature_set_5</td>\n",
       "      <td>0.892191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BO_result_rf_feature_set_1</td>\n",
       "      <td>0.89219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BO_result_gb_feature_set_2</td>\n",
       "      <td>0.89217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_adam_feature_set_1</td>\n",
       "      <td>0.892126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BO_result_gb_feature_set_1</td>\n",
       "      <td>0.892026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BO_result_gb_feature_set_7</td>\n",
       "      <td>0.891952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BO_result_rf_feature_set_5</td>\n",
       "      <td>0.891882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BO_result_rf_feature_set_2</td>\n",
       "      <td>0.890341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_adam_feature_set_2</td>\n",
       "      <td>0.8902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BO_result_svm_radial_feature_set_5</td>\n",
       "      <td>0.889473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_adam_feature_set_5</td>\n",
       "      <td>0.889301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_batch_feature_set_7</td>\n",
       "      <td>0.888391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BO_result_svm_linear_feature_set_5</td>\n",
       "      <td>0.88831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BO_result_svm_poly_feature_set_5</td>\n",
       "      <td>0.888212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_batch_feature_set_2</td>\n",
       "      <td>0.887966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_batch_feature_set_5</td>\n",
       "      <td>0.887895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>BO_result_EN_CATEGORICAL_feature_set_5</td>\n",
       "      <td>0.887155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>BO_result_svm_linear_feature_set_7</td>\n",
       "      <td>0.886297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BO_result_svm_poly_feature_set_7</td>\n",
       "      <td>0.886291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LOGISTIC_result_feature_set_7</td>\n",
       "      <td>0.885893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BO_result_svm_poly_feature_set_2</td>\n",
       "      <td>0.885713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>BO_result_svm_linear_feature_set_2</td>\n",
       "      <td>0.88569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BO_result_svm_linear_feature_set_1</td>\n",
       "      <td>0.885676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>BO_result_svm_poly_feature_set_1</td>\n",
       "      <td>0.885505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>BO_result_svm_radial_feature_set_2</td>\n",
       "      <td>0.884675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>LOGISTIC_result_feature_set_2</td>\n",
       "      <td>0.884323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LOGISTIC_result_feature_set_5</td>\n",
       "      <td>0.883817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>BO_result_svm_radial_feature_set_1</td>\n",
       "      <td>0.883267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_batch_feature_set_7</td>\n",
       "      <td>0.883219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>BO_result_svm_radial_feature_set_7</td>\n",
       "      <td>0.882606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LOGISTIC_result_feature_set_1</td>\n",
       "      <td>0.88214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>BO_result_MLP_1hiddenlayer_batch_feature_set_1</td>\n",
       "      <td>0.882116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_adam_feature_set_5</td>\n",
       "      <td>0.881093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>BO_result_knn_feature_set_5</td>\n",
       "      <td>0.880393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BO_result_EN_CATEGORICAL_feature_set_2</td>\n",
       "      <td>0.878975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>BO_result_MLP_2hiddenlayer_batch_feature_set_2</td>\n",
       "      <td>0.878834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>BO_result_EN_CATEGORICAL_feature_set_1</td>\n",
       "      <td>0.876459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>BO_result_EN_CATEGORICAL_feature_set_7</td>\n",
       "      <td>0.875869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NB_result_feature_set_2</td>\n",
       "      <td>0.873136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NB_result_feature_set_7</td>\n",
       "      <td>0.872332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NB_result_feature_set_5</td>\n",
       "      <td>0.854487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NB_result_feature_set_1</td>\n",
       "      <td>0.853837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>BO_result_knn_feature_set_2</td>\n",
       "      <td>0.852051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>BO_result_knn_feature_set_1</td>\n",
       "      <td>0.851528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>BO_result_knn_feature_set_7</td>\n",
       "      <td>0.843811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MODEL  cv_score\n",
       "0    BO_result_MLP_1hiddenlayer_adam_feature_set_7  0.901503\n",
       "1    BO_result_MLP_2hiddenlayer_adam_feature_set_7  0.898309\n",
       "2    BO_result_MLP_1hiddenlayer_adam_feature_set_1  0.897762\n",
       "3   BO_result_MLP_2hiddenlayer_batch_feature_set_1  0.894108\n",
       "4                       BO_result_gb_feature_set_5  0.893861\n",
       "5    BO_result_MLP_1hiddenlayer_adam_feature_set_2  0.893828\n",
       "6                       BO_result_rf_feature_set_7  0.892566\n",
       "7   BO_result_MLP_1hiddenlayer_batch_feature_set_5  0.892191\n",
       "8                       BO_result_rf_feature_set_1   0.89219\n",
       "9                       BO_result_gb_feature_set_2   0.89217\n",
       "10   BO_result_MLP_2hiddenlayer_adam_feature_set_1  0.892126\n",
       "11                      BO_result_gb_feature_set_1  0.892026\n",
       "12                      BO_result_gb_feature_set_7  0.891952\n",
       "13                      BO_result_rf_feature_set_5  0.891882\n",
       "14                      BO_result_rf_feature_set_2  0.890341\n",
       "15   BO_result_MLP_2hiddenlayer_adam_feature_set_2    0.8902\n",
       "16              BO_result_svm_radial_feature_set_5  0.889473\n",
       "17   BO_result_MLP_1hiddenlayer_adam_feature_set_5  0.889301\n",
       "18  BO_result_MLP_1hiddenlayer_batch_feature_set_7  0.888391\n",
       "19              BO_result_svm_linear_feature_set_5   0.88831\n",
       "20                BO_result_svm_poly_feature_set_5  0.888212\n",
       "21  BO_result_MLP_1hiddenlayer_batch_feature_set_2  0.887966\n",
       "22  BO_result_MLP_2hiddenlayer_batch_feature_set_5  0.887895\n",
       "23          BO_result_EN_CATEGORICAL_feature_set_5  0.887155\n",
       "24              BO_result_svm_linear_feature_set_7  0.886297\n",
       "25                BO_result_svm_poly_feature_set_7  0.886291\n",
       "26                   LOGISTIC_result_feature_set_7  0.885893\n",
       "27                BO_result_svm_poly_feature_set_2  0.885713\n",
       "28              BO_result_svm_linear_feature_set_2   0.88569\n",
       "29              BO_result_svm_linear_feature_set_1  0.885676\n",
       "30                BO_result_svm_poly_feature_set_1  0.885505\n",
       "31              BO_result_svm_radial_feature_set_2  0.884675\n",
       "32                   LOGISTIC_result_feature_set_2  0.884323\n",
       "33                   LOGISTIC_result_feature_set_5  0.883817\n",
       "34              BO_result_svm_radial_feature_set_1  0.883267\n",
       "35  BO_result_MLP_2hiddenlayer_batch_feature_set_7  0.883219\n",
       "36              BO_result_svm_radial_feature_set_7  0.882606\n",
       "37                   LOGISTIC_result_feature_set_1   0.88214\n",
       "38  BO_result_MLP_1hiddenlayer_batch_feature_set_1  0.882116\n",
       "39   BO_result_MLP_2hiddenlayer_adam_feature_set_5  0.881093\n",
       "40                     BO_result_knn_feature_set_5  0.880393\n",
       "41          BO_result_EN_CATEGORICAL_feature_set_2  0.878975\n",
       "42  BO_result_MLP_2hiddenlayer_batch_feature_set_2  0.878834\n",
       "43          BO_result_EN_CATEGORICAL_feature_set_1  0.876459\n",
       "44          BO_result_EN_CATEGORICAL_feature_set_7  0.875869\n",
       "45                         NB_result_feature_set_2  0.873136\n",
       "46                         NB_result_feature_set_7  0.872332\n",
       "47                         NB_result_feature_set_5  0.854487\n",
       "48                         NB_result_feature_set_1  0.853837\n",
       "49                     BO_result_knn_feature_set_2  0.852051\n",
       "50                     BO_result_knn_feature_set_1  0.851528\n",
       "51                     BO_result_knn_feature_set_7  0.843811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_results_array = pd.DataFrame(np.array([\"\",0,\"\",0]).reshape((2,2)), columns= [\"MODEL\",\"cv_score\"]   )\n",
    "\n",
    "models = [i for i in globals().keys() if (\"_result_\" in i) and ((\"_1\" in i) or (\"_2\" in i) or (\"_5\" in i) or (\"_7\" in i))]\n",
    "\n",
    "counter = 0\n",
    "for model in models:\n",
    "    \n",
    "    if \"cv_predictions\" not in model:    \n",
    "        cv_results_array.loc[counter] = [model,globals()[model].best_score_]\n",
    "        counter += 1\n",
    "        \n",
    "cv_results_array = cv_results_array.sort_values(by=\"cv_score\", ascending=False)        \n",
    "cv_results_array.index = list(range(cv_results_array.shape[0]))\n",
    "\n",
    "display(cv_results_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PREVIOUSLY SAVED CROSS-VALIDATED PREDICTIONS OF THE MODEL + CREATE AND SAVE THOSE THAT HAVE NOT BEEN CREATED YET\n",
    "\n",
    "### A DIFFERENT NON-REPEATED STRATIFIED 10-FOLD CV SCHEME IS USED TO GENERATE THE PREDICTIONS. IN SKLEARN, YOU CANNOT GENERATE PREDICTION VIA CV FOR REPEATED SCHEMES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PREVIOUSLY SAVED CV PREDICTIONS\n",
    "for file in listdir(getcwd()+\"/trained_models\"):\n",
    "    if file.endswith(\".pickle\") and \"cv_predictions\" in file:\n",
    "        name_object = re.sub(\".pickle\",\"\",file)\n",
    "        globals()[name_object] = joblib.load(\"trained_models/\"+file)\n",
    "        \n",
    "# CREATE AND SAVE THOSE WHICH HAVE NOT BEEN CREATED YET\n",
    "for model in cv_results_array[\"MODEL\"]:\n",
    "    if \"cv_predictions_\"+model not in globals():\n",
    "        feature_this = \"feature_set_\"+str(model[-1])\n",
    "        if feature_this == \"feature_set_0\":\n",
    "            feature_this = \"feature_set_10\"\n",
    "        X_train_this = X_train[:,globals()[feature_this]]\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        # it is impossible to use a repeated cv scheme with cross_val_predict. Thus, changed it to single 10-fold cv\n",
    "        prediction_cv_this = sklearn.model_selection.cross_val_predict(estimator = globals()[model].best_estimator_,\\\n",
    "                                                                       X = X_train_this,\\\n",
    "                                                                       y = y_train,\\\n",
    "                                                                       cv = StratifiedKFold(n_splits=10,random_state=1),\n",
    "                                                                       method = \"predict_proba\")[:,1]\n",
    "        warnings.filterwarnings(\"default\")\n",
    "        globals()[\"cv_predictions_\"+model] = prediction_cv_this\n",
    "        joblib.dump(prediction_cv_this,getcwd()+\"/trained_models/cv_predictions_\"+model+\".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEIGHTED RANK AVERAGE ENSAMBLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL MODELS WILL BE AVERAGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_models_index = [i for i in range(cv_results_array.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE PREDICTIONS FROM CV PREDICTIONS IN THE TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"prediction_train\" in globals().keys():\n",
    "    del prediction_train\n",
    "\n",
    "for model in list(cv_results_array[\"MODEL\"].iloc[chosen_models_index]):\n",
    "    prediction_this = globals()[\"cv_predictions_\"+model]\n",
    "    # prediction_this = np.log(prediction_this/(1- prediction_this))\n",
    "    prediction_this = np.array(prediction_this).reshape((X_train.shape[0],1))\n",
    "    if \"prediction_train\" not in globals().keys():\n",
    "        prediction_train = prediction_this\n",
    "    else:\n",
    "        prediction_train = np.append(prediction_train, prediction_this, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rank predictions train\n",
    "prediction_train_rank = pd.DataFrame(prediction_train.copy()).rank(axis = 0)/X_train.shape[0]\n",
    "\n",
    "# calculate weights according to cv performance\n",
    "weights_cv = np.array(cv_results_array.cv_score.iloc[chosen_models_index])\n",
    "\n",
    "# calculate weighted average rank predictions train\n",
    "prediction_train_rank_average = np.array(prediction_train_rank.apply(lambda x: np.average(x, weights=weights_cv), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE PREDICTION OF THE TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"prediction_test\" in globals().keys():\n",
    "    del prediction_test\n",
    "\n",
    "for model in list(cv_results_array[\"MODEL\"].iloc[chosen_models_index]):\n",
    "    feature = \"feature_set_\"+str(model[-1])\n",
    "    if feature == \"feature_set_0\":\n",
    "        feature = \"feature_set_10\"\n",
    "    prediction_this = globals()[model].predict_proba(X=X_test[:,globals()[feature]])[:,1]\n",
    "    prediction_this = np.array(prediction_this).reshape((X_test.shape[0],1))\n",
    "    if \"prediction_test\" not in globals().keys():\n",
    "        prediction_test = prediction_this\n",
    "    else:\n",
    "        prediction_test = np.append(prediction_test, prediction_this, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rank predictions test according to train set\n",
    "prediction_test_rank = pd.DataFrame(prediction_test.copy()).rank(axis = 0)/X_test.shape[0]\n",
    "\n",
    "for j in range(prediction_test_rank.shape[1]):\n",
    "    for i in range(prediction_test_rank.shape[0]):\n",
    "        new_value_index = np.where(np.abs(prediction_test[i,j] - prediction_train[:,j]) == np.min(np.abs(prediction_test[i,j] - prediction_train[:,j])))[0][0]\n",
    "        new_value = prediction_train_rank.iloc[new_value_index,j]\n",
    "        prediction_test_rank.iloc[i,j] = new_value\n",
    "\n",
    "# calculate weighted average rank predictions train\n",
    "prediction_test_rank_average = np.array(prediction_test_rank.apply(lambda x: np.average(x, weights=weights_cv), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING AVERAGE RANK\n",
      "__________________________________________________\n",
      "     \n",
      "TEST AUCROC SCORE: 0.859706959707\n",
      " \n",
      "THRESHOLD (MAXIMIZE BALANCED ACCURACY IN THE TRAIN SET): 0.5728\n",
      " \n",
      "BALANCED ACCURACY:                   0.745970695971\n",
      "F1-SCORE:                            0.68085106383\n",
      "SENSITIVITY/RECALL:                  0.820512820513\n",
      "SPECIFICITY:                         0.671428571429\n",
      "POSITIVE PREDICTIVE VALUE/PRECISION: 0.581818181818\n",
      "NEGATIVE PREDICTIVE VALUE:           0.87037037037\n",
      "     \n",
      "_______________________________________________\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "print(\"### TESTING AVERAGE RANK\")\n",
    "print(\"__________________________________________________\")\n",
    "print(\"     \")\n",
    "\n",
    "prediction_BEST_MODEL = prediction_test_rank_average\n",
    "print(\"TEST AUCROC SCORE: \"+str(roc_auc_score(y_true = y_test, y_score=prediction_BEST_MODEL)))\n",
    "print(\" \")\n",
    "\n",
    "np.savetxt(\"files/roc_test_wra.csv\",\\\n",
    "           np.array(roc_auc_score(y_true = y_test, y_score=prediction_BEST_MODEL)).reshape((1,1)),\\\n",
    "           delimiter=\"#\")\n",
    "\n",
    "t = calculate_best_threshold_train_dataset(prediction_train_calibration=prediction_train_rank_average, y_train_calibration = y_train)\n",
    "np.savetxt(\"files/t_best_balanaced_accuracy_wra.csv\",[t],delimiter=\"#\")\n",
    "\n",
    "# EXPORT TEST PREDICTIONS AND REFERENCES\n",
    "\n",
    "np.savetxt(\"files/probabilistic_prediction_test_wra.csv\", prediction_BEST_MODEL, delimiter=\",\")\n",
    "np.savetxt(\"files/categorical_outcome_test_wra.csv\", \\\n",
    "           [1 if i >= t else 0 for i in prediction_BEST_MODEL],\\\n",
    "           delimiter=\",\")\n",
    "\n",
    "print(\"THRESHOLD (MAXIMIZE BALANCED ACCURACY IN THE TRAIN SET): \"+ str(t))\n",
    "print(\" \")\n",
    "\n",
    "index_converters_test = np.where(y_test ==1)\n",
    "index_non_converters_test = np.where(y_test ==0)\n",
    "\n",
    "sens_test = sum(prediction_BEST_MODEL[index_converters_test] >= t) / prediction_BEST_MODEL[index_converters_test].shape[0]\n",
    "spec_test = sum(prediction_BEST_MODEL[index_non_converters_test] < t) / prediction_BEST_MODEL[index_non_converters_test].shape[0]\n",
    "ppv_test = sum(prediction_BEST_MODEL[index_converters_test] >= t) / sum(prediction_BEST_MODEL >= t)\n",
    "npv_test = sum(prediction_BEST_MODEL[index_non_converters_test] < t) / sum(prediction_BEST_MODEL < t)\n",
    "\n",
    "print(\"BALANCED ACCURACY:                   \"+ str((sens_test+spec_test)/2))\n",
    "print(\"F1-SCORE:                            \"+ str(2*ppv_test*sens_test/(ppv_test+sens_test)))\n",
    "print(\"SENSITIVITY/RECALL:                  \"+ str(sens_test))\n",
    "print(\"SPECIFICITY:                         \"+ str(spec_test))\n",
    "print(\"POSITIVE PREDICTIVE VALUE/PRECISION: \"+ str(ppv_test))\n",
    "print(\"NEGATIVE PREDICTIVE VALUE:           \"+ str(npv_test))\n",
    "print(\"     \")\n",
    "print(\"_______________________________________________\")\n",
    "print(\"     \")\n",
    "\n",
    "# MOVING SENSITIVITY LEVELS\n",
    "\n",
    "t_moving_results = calculate_parametrized_sensitivity_threshold_train_dataset(prediction_train_calibration=prediction_train_rank_average, y_train_calibration = y_train, desired_sensitivity_levels = [1.0,0.99,0.975,0.95,0.9,0.85,0.8,0.75])\n",
    "\n",
    "np.savetxt(\"files/t_moving_results_wra.csv\", t_moving_results, delimiter=\",\")\n",
    "for i in range(t_moving_results.shape[0]):\n",
    "    np.savetxt(\"files/categor_moving_sensitivity_outcome_test_\"+str(t_moving_results.iloc[i,0])+\"_wra.csv\", \\\n",
    "           [1 if pred >= t_moving_results.iloc[i,1] else 0 for pred in prediction_BEST_MODEL],\\\n",
    "           delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIVARIATE FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 1 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 100 | elapsed:    0.1s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# LOAD FEATURE IMPORTANCE WHICH HAVE ALREADY BEEN CREATED AND PICKLED\n",
    "\n",
    "for file in listdir(getcwd()+\"/univariate_feature_importance\"):\n",
    "    if file.endswith(\".pickle\") and \"UNIVARIATE_\" in file:\n",
    "        name_object = re.sub(\".pickle\",\"\",file)\n",
    "        globals()[name_object] = joblib.load(\"univariate_feature_importance/\"+file)\n",
    "\n",
    "\n",
    "# CREATE DATAFRAME OF RESULTS\n",
    "UNIVARIATE_FEATURE_IMPORTANCE = pd.DataFrame(np.array([\"\",0,\"\",0]).reshape((2,2)), columns= [\"FEATURE\",\"AUC_cv_score\"]   )\n",
    "\n",
    "# # TRAIN THE SINGLE FEATURE MODELS, ONLY FOR THOSE THAT WERE NOT TRAINED BEFORE\n",
    "\n",
    "for feature_this in feature_set_1:\n",
    "    \n",
    "    if \"UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this]) not in globals():\n",
    "        \n",
    "        search_this =  GridSearchCV(estimator=LogisticRegression(),\\\n",
    "                                            param_grid = dict(penalty = ['l2'], C = [sys.maxsize]),\\\n",
    "                                            cv=rkf,\\\n",
    "                                            verbose = 1,\\\n",
    "                                            n_jobs=-1,\\\n",
    "                                            scoring = scorer_this,\\\n",
    "                                            iid=False,\\\n",
    "                                            return_train_score=True)\n",
    "                \n",
    "        search_this.fit(X=X_train[:,feature_this].reshape((X_train.shape[0],1)), y=y_train)\n",
    "    \n",
    "        # SAVE IT IN THE GLOBAL ENVIRONMENT\n",
    "        globals()[\"UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this])] = search_this\n",
    "        \n",
    "        # DUMP THE MODEL\n",
    "        joblib.dump(search_this, \"univariate_feature_importance/UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this])+\".pickle\")\n",
    "    \n",
    "    if \"prediction_test_UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this]) not in globals():\n",
    "        search_this =  GridSearchCV(estimator=LogisticRegression(),\\\n",
    "                                            param_grid = dict(penalty = ['l2'], C = [sys.maxsize]),\\\n",
    "                                            cv=rkf,\\\n",
    "                                            verbose = 1,\\\n",
    "                                            n_jobs=-1,\\\n",
    "                                            scoring = scorer_this,\\\n",
    "                                            iid=False,\\\n",
    "                                            return_train_score=True)\n",
    "                \n",
    "        search_this.fit(X=X_train[:,feature_this].reshape((X_train.shape[0],1)), y=y_train)\n",
    "        \n",
    "        # GENERATE TEST PREDICTIONS\n",
    "        prediction_this = search_this.predict_proba(X=X_test[:,feature_this].reshape((X_test.shape[0],1)))[:,1]\n",
    "    \n",
    "        # SAVE THE PREDICTIONS\n",
    "        np.savetxt(getcwd()+\"/univariate_feature_importance/prediction_test_UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this])+\"_\"+str(fold)+\".csv\", prediction_this, delimiter=\",\")\n",
    "    \n",
    "    # UPDATE THE DATAFRAME OF REULTS\n",
    "    UNIVARIATE_FEATURE_IMPORTANCE.loc[feature_this] = [predictor_names[feature_this],\\\n",
    "                                                       globals()[\"UNIVARIATE_FEATURE_IMPORTANCE_\"+str(predictor_names[feature_this])].best_score_]\n",
    "    \n",
    "# SAVE RESULTS\n",
    "\n",
    "UNIVARIATE_FEATURE_IMPORTANCE.to_excel(\"UNIVARIATE_FEATURE_IMPORTANCE_fold_\"+str(fold)+\".xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE CV RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "cv_results_array = cv_results_array.sort_values(by=\"MODEL\", ascending=False)        \n",
    "        \n",
    "cv_results_array[\"technique\"] = \"\"\n",
    "for i in range(cv_results_array.shape[0]):\n",
    "    if \"NB\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Naïve Bayes\"\n",
    "    elif \"LOGISTIC\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Logistic Regression\"\n",
    "    elif \"svm_radial\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Support Vector Machine, polynomial kernel\"\n",
    "    elif \"svm_poly\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Support Vector Machine, rbf kernel\"\n",
    "    elif \"svm_linear\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Support Vector Machine, linear kernel\"\n",
    "    elif \"rf\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Random Forest\"\n",
    "    elif \"knn\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"k-Nearest Neighbor\"\n",
    "    elif \"gb\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Gradient Boosting Machine\"\n",
    "    elif \"MLP_1hiddenlayer_batch\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Multi-layer Perceptrion, 1 hidden layer and batch learning\"\n",
    "    elif \"MLP_2hiddenlayer_batch\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Multi-layer Perceptrion, 2 hidden layers and batch learning\"\n",
    "    elif \"MLP_1hiddenlayer_adam\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Multi-layer Perceptrion, 1 hidden layer and adam learning\"\n",
    "    elif \"MLP_2hiddenlayer_adam\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Multi-layer Perceptrion, 2 hidden layers and adam learning\"\n",
    "    elif \"EN_CATEGORICAL\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"technique\"].iloc[i] = \"Elastic Net\"\n",
    "\n",
    "cv_results_array[\"feature\"] = 0\n",
    "for i in range(cv_results_array.shape[0]):\n",
    "    if \"feature_set_1\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"feature\"].iloc[i] = 1\n",
    "    elif \"feature_set_2\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"feature\"].iloc[i] = 3\n",
    "    elif \"feature_set_5\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"feature\"].iloc[i] = 5\n",
    "    elif \"feature_set_7\" in cv_results_array[\"MODEL\"].iloc[i]:\n",
    "        cv_results_array[\"feature\"].iloc[i] = 7\n",
    "\n",
    "cv_results_array = cv_results_array.sort_values(by=[\"feature\",\"technique\"], ascending=False)        \n",
    "\n",
    "cv_results_array.to_excel(\"cv_results_fold_\"+str(fold)+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
